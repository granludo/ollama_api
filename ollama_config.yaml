# Description: Configuration file for Ollama
llm:
  url: "http://localhost:11434"
  instruct_endpoint: "/api/generate"
  chat_endpoint: "/api/chat"
  instruct_model: "mixtral"
  chat_model: "mixtral"
  temperature: 0.7
  headers:
    Content-Type: "application/json"
